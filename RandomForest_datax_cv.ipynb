{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\vishn\\anaconda3\\lib\\site-packages (3.8.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\vishn\\anaconda3\\lib\\site-packages (from gensim) (4.2.0)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\vishn\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\vishn\\anaconda3\\lib\\site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\vishn\\anaconda3\\lib\\site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied: Cython==0.29.14 in c:\\users\\vishn\\anaconda3\\lib\\site-packages (from gensim) (0.29.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/estella/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/estella/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/estella/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/estella/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /Users/estella/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import Beautiful Soup, NumPy and Pandas, etc\n",
    "import bs4 as bs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import hashlib\n",
    " \n",
    "# download NLTK classifiers - these are cached locally on your machine\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw')\n",
    "\n",
    "# import ml classifiers\n",
    "from nltk.tokenize import sent_tokenize # tokenizes sentences\n",
    "from nltk.stem import PorterStemmer     # parsing/stemmer\n",
    "from nltk.tag import pos_tag            # parts-of-speech tagging\n",
    "from nltk.corpus import wordnet         # sentiment scores\n",
    "from nltk.stem import WordNetLemmatizer # stem and context\n",
    "from nltk.corpus import stopwords       # stopwords\n",
    "from nltk.util import ngrams            # ngram iterator\n",
    "\n",
    "# import word2vec\n",
    "#from gensim.test.utils import datapath\n",
    "#from gensim import utils\n",
    "#from gensim.models import Word2Vec\n",
    "\n",
    "# import sklearn\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import normalize, FunctionTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def review_cleaner(review, lemmatize=True, stem=False):\n",
    "    '''\n",
    "        Clean and preprocess a review.\n",
    "            1. Remove HTML tags\n",
    "            2. Extract emoticons\n",
    "            3. Use regex to remove all special characters (only keep letters)\n",
    "            4. Make strings to lower case and tokenize / word split reviews\n",
    "            5. Remove English stopwords\n",
    "            6. Lemmatize\n",
    "            7. Rejoin to one string\n",
    "        \n",
    "        @review (type:str) is an unprocessed review string\n",
    "        @return (type:str) is a 6-step preprocessed review string\n",
    "    '''\n",
    "\n",
    "    \n",
    "\n",
    "    if lemmatize == True and stem == True:\n",
    "        raise RuntimeError(\"May not pass both lemmatize and stem flags\")\n",
    "\n",
    "    #1. Remove HTML tags\n",
    "    review = bs.BeautifulSoup(review,features='lxml').text\n",
    "\n",
    "    #2. Use regex to find emoticons\n",
    "    emoticons = re.findall('(:D|:\\/)(?=\\s|[^[:alnum:]+-]|$)', review)\n",
    "\n",
    "    #3. Remove punctuation\n",
    "    review = re.sub('[^a-zA-Z ]' ,'',review)\n",
    "\n",
    "    #4. Tokenize into words (all lower case)\n",
    "    review_words = (str.lower(review.replace('.','. '))).split()\n",
    "\n",
    "    #5. Remove stopwords, Lemmatize, Stem\n",
    "    \n",
    "    review_wo_stopwords = [w for w in review_words if not w in eng_stopwords]\n",
    "    \n",
    "    token_tag = pos_tag(review_wo_stopwords)\n",
    "    NN_count = 0\n",
    "    JJ_count = 0\n",
    "\n",
    "    for pair in token_tag:\n",
    "        tag = pair[1]\n",
    "        if tag == 'JJ':\n",
    "            JJ_count+=1\n",
    "        elif tag == 'NN':\n",
    "            NN_count+=1\n",
    "        \n",
    "     \n",
    "    def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return 'n'\n",
    "    \n",
    "    wnl_stems = []\n",
    "    \n",
    "    for pair in token_tag:\n",
    "        res = wnl.lemmatize(pair[0],pos=get_wordnet_pos(pair[1]))\n",
    "        wnl_stems.append(res)\n",
    "    \n",
    "    for i in emoticons:\n",
    "        wnl_stems.append(i)\n",
    "    \n",
    "    #6. Join the review to one sentence\n",
    "    review_processed = ' '.join(wnl_stems)\n",
    "    \n",
    "    return review_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We vectorize the text using a bag of words model\n",
    "def get_vectorizer(ngram, max_features):\n",
    "    return CountVectorizer(ngram_range=(1, ngram),\n",
    "                             analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = review_cleaner,\n",
    "                             stop_words = None, \n",
    "                             max_features = max_features)\n",
    "\n",
    "# Model training\n",
    "def train_predict_sentiment(reviews, vectorizer, y=train[\"type\"], ngram=1, max_features=1000, model_random_state=0):\n",
    "    '''\n",
    "        This function will:\n",
    "            1. split data into train and test set.\n",
    "            2. get n-gram counts from cleaned reviews \n",
    "            3. train a random forest model using train n-gram counts and y (labels)\n",
    "            4. test the model on your test split\n",
    "            5. print accuracy of sentiment prediction on test and training data\n",
    "            6. print confusion matrix on test data results\n",
    "\n",
    "            To change n-gram type, set value of ngram argument\n",
    "            To change the number of features you want the countvectorizer to generate, set the value of max_features argument\n",
    "            \n",
    "            @cleaned_review (type:str) is preprocessed string from review_cleaner()\n",
    "            @return none\n",
    "    '''\n",
    "\n",
    "    print(\"Creating the model!\\n\")\n",
    "    \n",
    "    # train / test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(reviews, y, random_state=0, test_size=.2)\n",
    "\n",
    "    # Then we use fit_transform() to fit the model / learn the vocabulary,\n",
    "    # then transform the data into feature vectors.\n",
    "    # The input should be a list of strings. .toarray() converts to a numpy array\n",
    "    \n",
    "    train_bag = vectorizer.fit_transform(X_train)\n",
    "    if not isinstance(train_bag, np.ndarray):\n",
    "        train_bag = train_bag.toarray()\n",
    "    test_bag = vectorizer.transform(X_test)\n",
    "    if not isinstance(test_bag, np.ndarray):\n",
    "        test_bag = test_bag.toarray()\n",
    "\n",
    "    print(\"Training the random forest classifier!\\n\")\n",
    "    # Initialize a Random Forest classifier with 50 trees\n",
    "    forest = RandomForestClassifier(n_estimators = 50, random_state = model_random_state) \n",
    "\n",
    "    # Fit the forest to the training set, using the bag of words as \n",
    "    # features and the sentiment labels as the target variable\n",
    "    forest = forest.fit(train_bag, y_train)\n",
    "\n",
    "    # predict\n",
    "    train_predictions = forest.predict(train_bag)\n",
    "    test_predictions = forest.predict(test_bag)\n",
    "    \n",
    "    # validation\n",
    "    train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "    valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "    print(\" The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)\n",
    "    print()\n",
    "    print('CONFUSION MATRIX:')\n",
    "    print('         Predicted')\n",
    "    print('          neg pos')\n",
    "    print(' Actual')\n",
    "    c=confusion_matrix(y_test, test_predictions)\n",
    "    print('     neg  ',c[0])\n",
    "    print('     pos  ',c[1])\n",
    "\n",
    "    return forest\n",
    "\n",
    "#fhttps://stackoverflow.com/questions/57340142/user-warning-your-stop-words-may-be-inconsistent-with-your-preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"news_sample.csv\", sep=\",\").iloc[:,[3,5]].dropna()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['content'], train['type'], random_state=0, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kfold validation, stratification causes an error in splitting y\n",
    "kf = KFold(5, shuffle=True,random_state=0)\n",
    "\n",
    "#Pipeline model with vectorizer first\n",
    "text_pipe = Pipeline([('tfid', TfidfVectorizer()),\n",
    "                       ('rf', RandomForestClassifier( random_state = 0))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    }
   ],
   "source": [
    "#cross validation\n",
    "cvparams = {'tfid__ngram_range' : [(1, 1), (1,2), (2,2)], #unigrams, uni and bi, only bigrams\n",
    "            'tfid__stop_words': [None],\n",
    "            'tfid__preprocessor': [review_cleaner],\n",
    "            #'tfid__max_df': [0.8, 1], #When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words\n",
    "            #'tfid__min_df':[0.01,0.001], #When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold\n",
    "            'rf__max_depth': [500, 1000],\n",
    "            'rf__min_samples_split': [3,5],\n",
    "            'rf__min_samples_leaf': [1,5,8]}\n",
    "    \n",
    "            #'rf__n_estimators': (20,50,100)}#,\n",
    "          #'rf__ccp_alpha': [0.0005, 0.001, 0.002, 0.01], #pruning cutoff\n",
    "          #'rf__max_depth': range(3,6), #depth of tree\n",
    "          #'#rf__max_features': range(7,15)} #max number of features to bootstrap per tree}\n",
    "            \n",
    "cv_rf = GridSearchCV(text_pipe, param_grid = cvparams, cv = kf, verbose = 1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__max_depth': 500,\n",
       " 'rf__min_samples_leaf': 1,\n",
       " 'rf__min_samples_split': 2,\n",
       " 'tfid__ngram_range': (1, 1),\n",
       " 'tfid__preprocessor': <function __main__.review_cleaner(review, lemmatize=True, stem=False)>,\n",
       " 'tfid__stop_words': None}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6842105263157896"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_rf.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The training accuracy is:  1.0 \n",
      " The test accuracy is:  0.8125\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [0 0 0 0 1 0 0 0]\n",
      "     pos   [0 4 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "bp = cv_rf.best_params_ \n",
    "#initial cv\n",
    "#{'rf__max_depth': 500,\n",
    "# 'rf__min_samples_leaf': 1,\n",
    "# 'rf__min_samples_split': 2,\n",
    "# 'tfid__ngram_range': (1, 1),\n",
    " #'tfid__preprocessor': review_cleaner,\n",
    "# 'tfid__stop_words': None}\n",
    "\n",
    "\n",
    "bestforest =  Pipeline([('tfid', TfidfVectorizer(ngram_range = bp[\"tfid__ngram_range\"],\n",
    "                                                 preprocessor = review_cleaner)),\n",
    "                                                 #max_df = bp['tfid__max_df'],\n",
    "                                                 #min_df = bp['tfid__min_df'])),\n",
    "                       ('rf', RandomForestClassifier(max_depth = bp[\"rf__max_depth\"],\n",
    "                                                     min_samples_split = bp[\"rf__min_samples_split\"],\n",
    "                                                     min_samples_leaf = bp[\"rf__min_samples_leaf\"],\n",
    "                                                     random_state = 0))]).fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "train_predictions = bestforest.predict(X_train)\n",
    "test_predictions = bestforest.predict(X_test)\n",
    "    \n",
    "# validation\n",
    "train_acc = metrics.accuracy_score(y_train, train_predictions)\n",
    "valid_acc = metrics.accuracy_score(y_test, test_predictions)\n",
    "    \n",
    "print(\" The training accuracy is: \", train_acc, \"\\n\", \"The test accuracy is: \", valid_acc)\n",
    "print()\n",
    "print('CONFUSION MATRIX:')\n",
    "print('         Predicted')\n",
    "print('          neg pos')\n",
    "print(' Actual')\n",
    "c=confusion_matrix(y_test, test_predictions)\n",
    "print('     neg  ',c[0])\n",
    "print('     pos  ',c[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fake']\n"
     ]
    }
   ],
   "source": [
    "phrase = [\"Space lasers cause forest fires\"]\n",
    "output = bestforest.predict(phrase)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the model!\n",
      "\n",
      "Training the random forest classifier!\n",
      "\n",
      " The training accuracy is:  0.9947368421052631 \n",
      " The validation accuracy is:  0.8125\n",
      "\n",
      "CONFUSION MATRIX:\n",
      "         Predicted\n",
      "          neg pos\n",
      " Actual\n",
      "     neg   [0 1 0 0 0 0 0 0]\n",
      "     pos   [0 5 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = get_vectorizer(ngram=1, max_features=100)\n",
    "forest_model = train_predict_sentiment(train['content'] , vectorizer=vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unreliable']\n"
     ]
    }
   ],
   "source": [
    "phrase = \"Space lasers cause forest fires\"\n",
    "output = forest_model.predict(vectorizer.transform([phrase]))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
